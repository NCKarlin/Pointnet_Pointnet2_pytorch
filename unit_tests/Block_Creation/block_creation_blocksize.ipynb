{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Block Creation based on Block Size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall description coming..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For paths\n",
    "import os \n",
    "\n",
    "# For data processing\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import math\n",
    "\n",
    "\n",
    "# For 3D visualization\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import proj3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and variables to be used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the root for this repo\n",
    "ROOT = os.path.realpath(\"..\")\n",
    "\n",
    "# Creating/ Making the point cloud\n",
    "def makePC(point_data, color_data=np.array([])):\n",
    "    pcd = o3d.geometry.PointCloud() #Create PC object\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_data) #Give coordinates\n",
    "    #Coloring the PC\n",
    "    if len(color_data) == 0:\n",
    "        pcd.paint_uniform_color([1, 0, 0])\n",
    "    else:\n",
    "        pcd.colors = o3d.utility.Vector3dVector(color_data)\n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting processing ot doing anything significant we will have to load in the input data to start investigating and testing. After the loading in of the data we print out certain characteristics of the respective sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path creation for easier loading\n",
    "input_pc_path = ROOT + \"/data/testdata/data_labelled_int.npy\"\n",
    "\n",
    "# Loading relevant data\n",
    "input_pc = np.load(input_pc_path) #ndarray\n",
    "input_pc_points, input_pc_labels = input_pc[:, 0:6], input_pc[:, 6]\n",
    "\n",
    "# Print checks for the input data\n",
    "print(\"--------- Print checks for input data ---------\")\n",
    "print(f\"The input data consists of {len(input_pc)} points.\")\n",
    "print(f\"The input data is of shape: {input_pc.shape}\")\n",
    "print(f\"The input data contains {np.count_nonzero(input_pc_labels)} fracture points.\")\n",
    "print(f\"Therefore, {len(input_pc) - np.count_nonzero(input_pc_labels)} points are non-fracture points in the entire sample.\")\n",
    "print(f\"This means that {np.round(np.count_nonzero(input_pc_labels) / len(input_pc) * 100, 4)}% of all points are fracture points\")\n",
    "print(\"------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block center coordinate creation based on block size parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above the block size is not really used for the block and center coordinate creation, therefore it will be adjusted according to the block size, while keeping the half-block-size padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given HP and parameters\n",
    "num_point = 4096\n",
    "block_size = 20.0\n",
    "sample_rate = 1.0\n",
    "\n",
    "# Determining maximum and minimum points\n",
    "coord_min, coord_max = np.amin(input_pc_points, axis=0)[:3], np.max(input_pc_points, axis=0)[:3]\n",
    "\n",
    "# Determining maximum block resolution\n",
    "max_blocks_x_res = math.ceil(coord_max[0] / block_size) \n",
    "max_blocks_y_res = math.ceil(coord_max[1] / block_size)\n",
    "# Actual resolution after half block padding on each side\n",
    "blocks_x_res = max_blocks_x_res - 1\n",
    "blocks_y_res = max_blocks_y_res - 1\n",
    "num_blocks = blocks_x_res * blocks_y_res\n",
    "blocks_center_x_start = coord_min[0] + block_size\n",
    "blocks_center_y_start = coord_min[1] + block_size\n",
    "#block_step = block_size\n",
    "\n",
    "# Center coordinate creation\n",
    "blocks_center_x_coords = np.linspace(start=blocks_center_x_start, \n",
    "                                     stop=coord_max[0]-block_size, \n",
    "                                     num=blocks_x_res)\n",
    "blocks_center_y_coords = np.linspace(start=blocks_center_y_start, \n",
    "                                     stop=coord_max[1]-block_size, \n",
    "                                     num=blocks_y_res)\n",
    "blocks_center_xx_coords, blocks_center_yy_coords = np.meshgrid(blocks_center_x_coords, \n",
    "                                                               blocks_center_y_coords)\n",
    "blocks_center_zz_coords = np.zeros((blocks_center_xx_coords.shape))\n",
    "blocks_center_coords = np.hstack([blocks_center_xx_coords.reshape((-1,1)), \n",
    "                                  blocks_center_yy_coords.reshape((-1,1)), \n",
    "                                  blocks_center_zz_coords.reshape((-1,1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have created the block center coordinates based on the block size parameter for this model. As a next step we now have to (double-)check whether, if blocked like this we might be able to get even nearly as many points as we need per block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of block creation based on block size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coming..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder variables to fill with values \n",
    "block_point_idxs = [] #actual indices of points of input pc within respective block\n",
    "block_num_frac_points = [] #actual labels of points of input pc within respective block\n",
    "block_num_input_pc_points = [] #number of points of input pc in respective block\n",
    "block_mins = [] #block minimum coordinate\n",
    "block_maxs = [] #block maximum coordiante\n",
    "\n",
    "\n",
    "# Looping through the created block centers to check for number of points\n",
    "for index, block_center in enumerate(blocks_center_coords):\n",
    "    block_min = block_center[:2] - [block_size / 2.0, block_size / 2.0]\n",
    "    block_max = block_center[:2] + [block_size / 2.0, block_size / 2.0]\n",
    "    # Determining the number of points in the block\n",
    "    points_in_block = np.where((input_pc_points[:, 0] >= block_min[0]) & \\\n",
    "                               (input_pc_points[:, 0] <= block_max[0]) & \\\n",
    "                               (input_pc_points[:, 1] >= block_min[1]) & \\\n",
    "                               (input_pc_points[:, 1] <= block_max[1]))[0]\n",
    "    #Pulling corresponding values for each block for analysis\n",
    "    selected_points_idxs = input_pc_points[points_in_block]\n",
    "    block_point_idxs.append(selected_points_idxs)\n",
    "    selected_points_labels = input_pc_labels[points_in_block]\n",
    "    block_num_frac_points.append(np.sum(selected_points_labels))\n",
    "    block_num_input_pc_points.append(points_in_block.size)\n",
    "    block_mins.append(block_min)\n",
    "    block_maxs.append(block_max)\n",
    "\n",
    "\n",
    "# Preparation for the print checks of the input pc\n",
    "points_in_blocks_with_enough_points = [i for i in block_num_input_pc_points if i >= num_point]\n",
    "idxs_blocks_with_enough_points = np.array([idx for idx, val in enumerate(block_num_input_pc_points) if val >= num_point], dtype=int)\n",
    "points_in_blocks_with_fewer_points = [i for i in block_num_input_pc_points if i < num_point]\n",
    "idxs_blocks_with_fewer_points = np.array([idx for idx, val in enumerate(block_num_input_pc_points) if val < num_point], dtype=int)\n",
    "num_blocks_without_any_points = [i for i in block_num_input_pc_points if i == 0]\n",
    "indices_of_blocks_with_zero_points = [idx for idx, val in enumerate(block_num_input_pc_points) if val == 0]\n",
    "num_blocks_with_too_little_points = num_blocks - len(points_in_blocks_with_enough_points)\n",
    "general_block_frac_point_avg = np.round(np.mean(block_num_frac_points), 2)\n",
    "blocks_with_enough_points_frac_point_avg = np.round(np.mean(np.array(block_num_frac_points)[idxs_blocks_with_enough_points]), 2)\n",
    "blocks_with_fewer_points_frac_point_avg = np.round(np.mean(np.array(block_num_frac_points)[idxs_blocks_with_fewer_points]), 2)\n",
    "\n",
    "\n",
    "# Print checks for the blocks within the input pc\n",
    "print('---------- Print checks for the input pc blocks ----------')\n",
    "print(f'The input point cloud was divided into {len(blocks_center_coords)} blocks.')\n",
    "print(f'The number of blocks that have more points than the subsampling goal: {len(points_in_blocks_with_enough_points)}')\n",
    "print(f'The number of blocks that have less points then the subsampling goal: {num_blocks_with_too_little_points}')\n",
    "print(f'The number of blocks which have no subsampled points at all: {len(indices_of_blocks_with_zero_points)}')\n",
    "print(f'The maximum amount of points within a block is: {np.max(points_in_blocks_with_enough_points)}')\n",
    "print(f'The average amount of points of the blocks that have enough points is: {np.round(np.mean(points_in_blocks_with_enough_points), 2)}')\n",
    "print(f'The average amount of points of the blocks that have to few points is: {np.round(np.mean(points_in_blocks_with_fewer_points), 2)}')\n",
    "print(f'On average there are {general_block_frac_point_avg} fracture points within a block.')\n",
    "print(f'On average the blocks with enough points have {blocks_with_enough_points_frac_point_avg} fracture points within a block.')\n",
    "print(f'On average the blocks with fewer points have {blocks_with_fewer_points_frac_point_avg} fracture points within a block.')\n",
    "print(f'The amount of blocks that do not contain any fracture points are: {block_num_frac_points.count(0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this investigation with the parameters set in this fashion we end up with almost roughly half the amount of blocks having enough points (80), but essentially a lot more points on average (8414), while the average number of points in blocks with too few points seems too low to let it go into the model (645). Besides the amount of fracture points also differs greatly between these two groups and as well. Furthermore, with the parameters set like this there seem to be 60 blocks, that do not even showcase any fracture points, while 40 blocks do not even contain any points. As this sounds very intriguing but almost not possible, we will check this below to make sure it is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Analyze the blocks with no points at all and also no fracture points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipynb-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
