{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was created in order to investigate the input point cloud to the model in all its detail. Mainly, this is done to make sure that the input data we provide is pristine and exactly what we imagine it to be like. Therefore, this notebook will guide everyone through the theoretical relevance and also the practical implementation of all the transformations, so we can at least rule out a bad dataset as a reason for suboptimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, Directory Roots and Statics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a quick reminder, in this repo and environment, the notebook is supposed to be run in the \"base\" environment, so that it selects the appropriate and working ekrnel for the python ntoebook structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed packages\n",
    "import torch\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "# Setting correct directory roots for the data\n",
    "DATA_ROOT = \"/home/innolidix/Pointnet_Pointnet2_pytorch/data/testdata\"\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the existing structure of the input point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to dwelve off and creating own new datasets, we firstly want to focus on inspecting the dataset created to make sure it is either correct or incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we check the data loading. This includes the loading in of the data and the subsequent distribution of points and labels, and accordingly also the labelweights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating path to input point cloud\n",
    "input_data_path = os.path.join(DATA_ROOT, \"data_labelled_int.npy\")\n",
    "\n",
    "# Loading in the current labelled test data\n",
    "input_data = np.load(input_data_path)\n",
    "\n",
    "# Splitting points and labels\n",
    "points, labels = input_data[:, 0:6], input_data[:, 6]\n",
    "\n",
    "# Getting counts of labelled points\n",
    "labelweights = np.zeros(2)\n",
    "tmp, _ = np.histogram(labels, range(3))\n",
    "labelweights += tmp\n",
    "# Creating labelweights\n",
    "model_labelweights = np.sum(labelweights) / labelweights\n",
    "\n",
    "# Retrieving minimum and maximum of coordinates for this file\n",
    "coord_min, coord_max = np.amin(points, axis=0)[:3], np.amax(points, axis=0)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have completed the first basic operations. It is important to keep in mind, that the data we are using for the overtraining currently is already transformed. Thereby, it is meant, that the point cloud itself already has been rotated and moved. Hence, the minimum one can observe is 0, while the maximum is a number. As a next step we do now take a look at the operations, that happen in the *__getitem__* method so we perfectly recreate the data used in the model currently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *__getitem__* transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the magic getitem method there are some operations and transformations taking place. Mainly, the following are carried out: <br>\n",
    "- Normalization of the point cloud coordinates to a randomly chosen center <br>\n",
    "- Normalization of the RGB values of the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([288.875869, 226.199528, 177.267736, 255.      , 255.      ,\n",
       "       255.      ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables needed\n",
    "N_points = points.shape[0]\n",
    "block_size = 4.0\n",
    "num_point = 4096\n",
    "\n",
    "# TRADITIONAL SPACIAL NORMALIZATION AFTER SAMPLING\n",
    "while(True):\n",
    "    center = points[np.random.choice(N_points)][:3]\n",
    "    # Creating block minimum and maximum\n",
    "    block_min = center - [block_size / 2.0, block_size / 2.0, 0]\n",
    "    block_max = center + [block_size / 2.0, block_size / 2.0, 0]\n",
    "    # Pulling indices of points within the boundaries\n",
    "    point_idxs = np.where((points[:,0] >= block_min[0]) &\n",
    "                          (points[:,0] <= block_max[0]) &\n",
    "                          (points[:,1] >= block_min[1]) &\n",
    "                          (points[:,1] <= block_max[1]))[0]\n",
    "    if point_idxs.size > 1024:\n",
    "        break\n",
    "\n",
    "# (Sub-) Sampling points from input point cloud data\n",
    "if point_idxs.size >= num_point:\n",
    "    selected_point_idxs = np.random.choice(point_idxs, num_point, replace=False)\n",
    "else:\n",
    "    selected_point_idxs = np.random.choice(point_idxs, num_point, replace=True)\n",
    "\n",
    "# SPACIAL NORMALIZATION according to randomly selected center point\n",
    "selected_points = points[selected_point_idxs, :]\n",
    "current_points = np.zeros((num_point, 9))\n",
    "# Shifting of X and Y coordinate according to randomly selected center\n",
    "selected_points[:, 0] = selected_points[:, 0] - center[0]\n",
    "selected_points[:, 1] = selected_points[:, 1] - center[1]\n",
    "\n",
    "# RGB NORMALIZATION\n",
    "selected_points[:, 3:6] /= 255\n",
    "\n",
    "# creating new rows corresponding to spacially normalized values\n",
    "current_points[:, 6] = selected_points[:, 0] / coord_max[0]\n",
    "current_points[:, 7] = selected_points[:, 1] / coord_max[1]\n",
    "current_points[:, 8] = selected_points[:, 2] / coord_max[2]\n",
    "\n",
    "# Piecing it together to return selected current points\n",
    "current_points[:, 0:6] = selected_points\n",
    "current_labels = labels[selected_point_idxs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted new getitem transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below here, you can find the adjusted transformations in the getitem method. Mainly, this has to do with getting rid of all the random choices, that are implemented there. Thus, we want to randomly select,but still select the same over and over again. Hence, we might insert a random seed to ensure that this is happening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([654326.,   8199.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
