{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was created in order to investigate the input point cloud to the model in all its detail. Mainly, this is done to make sure that the input data we provide is pristine and exactly what we imagine it to be like. Therefore, this notebook will guide everyone through the theoretical relevance and also the practical implementation of all the transformations, so we can at least rule out a bad dataset as a reason for suboptimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, Directory Roots and Statics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just a quick reminder, in this repo and environment, the notebook is supposed to be run in the \"base\" environment, so that it selects the appropriate and working ekrnel for the python ntoebook structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed packages\n",
    "import torch\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "# Setting correct directory roots for the data\n",
    "DATA_ROOT = \"/home/innolidix/Pointnet_Pointnet2_pytorch/data/testdata\"\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the existing structure of the input point cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to dwelve off and creating own new datasets, we firstly want to focus on inspecting the dataset created to make sure it is either correct or incorrect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we check the data loading. This includes the loading in of the data and the subsequent distribution of points and labels, and accordingly also the labelweights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating path to input point cloud\n",
    "input_data_path = os.path.join(DATA_ROOT, \"data_labelled_int.npy\")\n",
    "\n",
    "# Loading in the current labelled test data\n",
    "input_data = np.load(input_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Creation Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under this block one can find all the dataset transformations as of now. These should be run before trying to run any getitem block, that tries to adjust the point selection. Mainly, what is happening here is: <br>\n",
    "- the loaded data is split up into data and labels\n",
    "- coordinate minimum and maximum is determined and saved for each block\n",
    "- labelweights for the respective model are determined and saved\n",
    "- number of points within the block is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting points and labels\n",
    "points, labels = input_data[:, 0:6], input_data[:, 6]\n",
    "\n",
    "# Getting counts of labelled points\n",
    "labelweights = np.zeros(2)\n",
    "tmp, _ = np.histogram(labels, range(3))\n",
    "labelweights += tmp\n",
    "# Creating labelweights\n",
    "model_labelweights = np.sum(labelweights) / labelweights\n",
    "\n",
    "# Retrieving minimum and maximum of coordinates for this file\n",
    "coord_min, coord_max = np.amin(points, axis=0)[:3], np.amax(points, axis=0)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have completed the first basic operations. It is important to keep in mind, that the data we are using for the overtraining currently is already transformed. Thereby, it is meant, that the point cloud itself already has been rotated and moved. Hence, the minimum one can observe is 0, while the maximum is a number. As a next step we do now take a look at the operations, that happen in the *__getitem__* method so we perfectly recreate the data used in the model currently. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *__getitem__* transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the magic getitem method there are some operations and transformations taking place. Mainly, the following are carried out: <br>\n",
    "- Normalization of the point cloud coordinates to a randomly chosen center <br>\n",
    "- Normalization of the RGB values of the points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([288.875869, 226.199528, 177.267736, 255.      , 255.      ,\n",
       "       255.      ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variables needed\n",
    "N_points = points.shape[0]\n",
    "block_size = 4.0\n",
    "num_point = 4096\n",
    "\n",
    "# TRADITIONAL SPACIAL NORMALIZATION AFTER SAMPLING\n",
    "while(True):\n",
    "    center = points[np.random.choice(N_points)][:3]\n",
    "    # Creating block minimum and maximum\n",
    "    block_min = center - [block_size / 2.0, block_size / 2.0, 0]\n",
    "    block_max = center + [block_size / 2.0, block_size / 2.0, 0]\n",
    "    # Pulling indices of points within the boundaries\n",
    "    point_idxs = np.where((points[:,0] >= block_min[0]) &\n",
    "                          (points[:,0] <= block_max[0]) &\n",
    "                          (points[:,1] >= block_min[1]) &\n",
    "                          (points[:,1] <= block_max[1]))[0]\n",
    "    if point_idxs.size > 1024:\n",
    "        break\n",
    "\n",
    "# (Sub-) Sampling points from input point cloud data\n",
    "if point_idxs.size >= num_point:\n",
    "    selected_point_idxs = np.random.choice(point_idxs, num_point, replace=False)\n",
    "else:\n",
    "    selected_point_idxs = np.random.choice(point_idxs, num_point, replace=True)\n",
    "\n",
    "# SPACIAL NORMALIZATION according to randomly selected center point\n",
    "selected_points = points[selected_point_idxs, :]\n",
    "current_points = np.zeros((num_point, 9))\n",
    "# Shifting of X and Y coordinate according to randomly selected center\n",
    "selected_points[:, 0] = selected_points[:, 0] - center[0]\n",
    "selected_points[:, 1] = selected_points[:, 1] - center[1]\n",
    "\n",
    "# RGB NORMALIZATION\n",
    "selected_points[:, 3:6] /= 255\n",
    "\n",
    "# creating new rows corresponding to spacially normalized values\n",
    "current_points[:, 6] = selected_points[:, 0] / coord_max[0]\n",
    "current_points[:, 7] = selected_points[:, 1] / coord_max[1]\n",
    "current_points[:, 8] = selected_points[:, 2] / coord_max[2]\n",
    "\n",
    "# Piecing it together to return selected current points\n",
    "current_points[:, 0:6] = selected_points\n",
    "current_labels = labels[selected_point_idxs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted new getitem transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below here, you can find the adjusted transformations in the getitem method. Mainly, this has to do with getting rid of all the random choices, that are implemented there. Thus, we want to randomly select,but still select the same over and over again. Hence, we might insert a random seed to ensure that this is happening. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New standardized point selection for block points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to stabilize the data inflow, I am now designing a new block points selection method. By default it will look and try and find points within the true center of a block, and if it does not find them it will iterate from top left to bottom right and choose the quadrant center which delivers the most points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There will be 2786 duplicate points in this block.\n"
     ]
    }
   ],
   "source": [
    "# Variables needed\n",
    "N_points = points.shape[0]\n",
    "block_size = 8.0\n",
    "num_point = 4096\n",
    "\n",
    "# Picking with true middle of the input point cloud\n",
    "center = np.mean(points, axis=0)[:3]\n",
    "block_min = center - [block_size / 2.0, block_size / 2.0, 0]\n",
    "block_max = center + [block_size / 2.0, block_size / 2.0, 0]\n",
    "point_idxs = np.where((points[:, 0] >= block_min[0]) & (points[:, 0] <= block_max[0]) & (points[:, 1] >= block_min[1]) & (points[:, 1] <= block_max[1]))[0]\n",
    "\n",
    "# If not enonugh points are found search for new center\n",
    "if point_idxs.size < 1024:\n",
    "    # Creating off centers\n",
    "    off_centers = np.array([\n",
    "        [0.25 * coord_max[0], 0.25 * coord_max[1], center[2]], \n",
    "        [0.25 * coord_max[0], 0.5 * coord_max[1], center[2]], \n",
    "        [0.25 * coord_max[0], 0.75 * coord_max[1], center[2]], \n",
    "        [0.5 * coord_max[0], 0.25 * coord_max[1], center[2]],  \n",
    "        [0.5 * coord_max[0], 0.75 * coord_max[1], center[2]], \n",
    "        [0.75 * coord_max[0], 0.25 * coord_max[1], center[2]], \n",
    "        [0.75 * coord_max[0], 0.5 * coord_max[1], center[2]], \n",
    "        [0.75 * coord_max[0], 0.75 * coord_max[1], center[2]], \n",
    "    ])\n",
    "    # Creation of list to pick newly sampled points from\n",
    "    points_idxs_list = []\n",
    "    points_len_list = []\n",
    "    for off_center in off_centers:\n",
    "        block_min = off_center - [block_size / 2.0, block_size / 2.0, 0]\n",
    "        block_max = off_center + [block_size / 2.0, block_size / 2.0, 0]\n",
    "        point_idxs = np.where((points[:, 0] >= block_min[0]) & (points[:, 0] <= block_max[0]) & (points[:, 1] >= block_min[1]) & (points[:, 1] <= block_max[1]))[0]\n",
    "        points_idxs_list.append(point_idxs)\n",
    "        points_len_list.append(len(point_idxs))\n",
    "    # Checking for any center with more than 1024 points within the block\n",
    "    if any(point_len>1024 for point_len in points_len_list):\n",
    "        # Grabbing center with most points\n",
    "        points_len_list_max_idx = np.argmax(points_len_list)\n",
    "        point_idxs = points_idxs_list[points_len_list_max_idx]\n",
    "        center = off_centers[points_len_list_max_idx]\n",
    "    else:\n",
    "        # Random point selection as before for now\n",
    "        while (True):\n",
    "            center = points[np.random.choice(N_points)][:3] #taking a completely random point in the room as centre\n",
    "            block_min = center - [block_size / 2.0, block_size / 2.0, 0] #half a blocksize to negative side of centre\n",
    "            block_max = center + [block_size / 2.0, block_size / 2.0, 0] #half a blocksize to positive side of centre\n",
    "            #getting indexes of the points that are within the block xy range\n",
    "            point_idxs = np.where((points[:, 0] >= block_min[0]) & (points[:, 0] <= block_max[0]) & (points[:, 1] >= block_min[1]) & (points[:, 1] <= block_max[1]))[0]\n",
    "            #if there are less than 1024 points in the block then choose a different center\n",
    "            if point_idxs.size > 1024:\n",
    "                break\n",
    "\n",
    "# Sampling the points from the input point cloud\n",
    "np.random.seed(42)\n",
    "if point_idxs.size >= num_point:\n",
    "    selected_point_idxs = np.random.choice(point_idxs, num_point, replace=False)\n",
    "else:\n",
    "    selected_point_idxs = np.random.choice(point_idxs, num_point, replace=True)\n",
    "    print(f\"There will be {num_point - point_idxs.size} duplicate points in this block.\")\n",
    "\n",
    "# SPACIAL NORMALIZATION\n",
    "selected_points = points[selected_point_idxs, :]\n",
    "selected_points[:, 0] = selected_points[:, 0] - center[0]\n",
    "selected_points[:, 1] = selected_points[:, 1] - center[1]\n",
    "current_points = np.zeros((num_point, 9))\n",
    "current_points[:, 6] = selected_points[:, 0] / coord_max[0]\n",
    "current_points[:, 7] = selected_points[:, 1] / coord_max[1]\n",
    "current_points[:, 8] = selected_points[:, 2] / coord_max[2]\n",
    "\n",
    "# RGB NNORMALIZATION\n",
    "selected_points[:, 3:6] /= 255.0\n",
    "\n",
    "# Piecing together\n",
    "current_points[:, 0:6] = selected_points\n",
    "current_labels = labels[selected_point_idxs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
